{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anticausative Verbs Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "anticausative_df = pd.read_csv(\"..\\\\Data\\\\df_50_causative_alt.csv\", index_col = \"Sent.id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sent.</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Token.id</th>\n",
       "      <th>Use</th>\n",
       "      <th>Token.Form</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sent.id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tony broke the window.</td>\n",
       "      <td>break</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "      <td>broke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The window broke.</td>\n",
       "      <td>break</td>\n",
       "      <td>2</td>\n",
       "      <td>intr</td>\n",
       "      <td>broke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tony bent the rod.</td>\n",
       "      <td>bend</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "      <td>bent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The trees bend during windy days.</td>\n",
       "      <td>bend</td>\n",
       "      <td>2</td>\n",
       "      <td>intr</td>\n",
       "      <td>bend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jennifer baked the potatoes.</td>\n",
       "      <td>bake</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "      <td>baked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Many revolve around the latest trendy silver b...</td>\n",
       "      <td>revolve</td>\n",
       "      <td>1</td>\n",
       "      <td>intr</td>\n",
       "      <td>revolve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>I smash it into boulders.</td>\n",
       "      <td>smash</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "      <td>smash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>A tentacle smashes into Koopark's head.</td>\n",
       "      <td>smash</td>\n",
       "      <td>2</td>\n",
       "      <td>intr</td>\n",
       "      <td>smashes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>We macerated for about 4 hours and then presse...</td>\n",
       "      <td>macerate</td>\n",
       "      <td>1</td>\n",
       "      <td>tr</td>\n",
       "      <td>macerated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The grapes macerated for 24 hours in stainless...</td>\n",
       "      <td>macerate</td>\n",
       "      <td>2</td>\n",
       "      <td>intr</td>\n",
       "      <td>macerated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Sent.     Lemma  \\\n",
       "Sent.id                                                                \n",
       "0                                   Tony broke the window.     break   \n",
       "1                                        The window broke.     break   \n",
       "2                                       Tony bent the rod.      bend   \n",
       "3                        The trees bend during windy days.      bend   \n",
       "4                             Jennifer baked the potatoes.      bake   \n",
       "...                                                    ...       ...   \n",
       "95       Many revolve around the latest trendy silver b...   revolve   \n",
       "96                               I smash it into boulders.     smash   \n",
       "97                 A tentacle smashes into Koopark's head.     smash   \n",
       "98       We macerated for about 4 hours and then presse...  macerate   \n",
       "99       The grapes macerated for 24 hours in stainless...  macerate   \n",
       "\n",
       "         Token.id   Use Token.Form  \n",
       "Sent.id                             \n",
       "0               1    tr      broke  \n",
       "1               2  intr      broke  \n",
       "2               1    tr       bent  \n",
       "3               2  intr       bend  \n",
       "4               1    tr      baked  \n",
       "...           ...   ...        ...  \n",
       "95              1  intr    revolve  \n",
       "96              1    tr      smash  \n",
       "97              2  intr    smashes  \n",
       "98              1    tr  macerated  \n",
       "99              2  intr  macerated  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anticausative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word pieces combination based on tokens ids\n",
    "def combine_subwords(ids, words):\n",
    "    \n",
    "    \"\"\"The function takes in two arguments both obtained with hf TokenizerFast class:\\\n",
    "    ids: a list of successive non-single ids \\\n",
    "    word_pieces: a list of word pieces\n",
    "    \n",
    "    return:\n",
    "    a dictionary mapping the ids to their respective subwords\n",
    "    a list of the reconstructed words\"\"\"\n",
    "    #remove the special char in bpe tokeniers\n",
    "    words = list(map(lambda x:x.replace(\"Ġ\", \"\").strip(), words))\n",
    "    #remove the special char for bert-like tokenizers\n",
    "    words = list(map(lambda x:x.replace(\"#\", \"\").strip(), words))\n",
    "    \n",
    "    # Ensure both input lists have the same length\n",
    "    if len(ids) != len(words):\n",
    "        raise ValueError(\"Input lists must have the same length\")\n",
    "\n",
    "    # Create a dictionary to store word pieces by id\n",
    "    id_word_dict = {}\n",
    "\n",
    "    # Iterate through the lists and populate the dictionary\n",
    "    for id, word in zip(ids, words):\n",
    "        if id not in id_word_dict:\n",
    "            id_word_dict[id] = []\n",
    "        id_word_dict[id].append(word)\n",
    "\n",
    "    # Create the list of tuples by joining the word pieces\n",
    "    result = [(id, ''.join(word_pieces)) for id, word_pieces in id_word_dict.items() if not id == None]\n",
    "\n",
    "    #get rid of None key if any. If present is for the special tokens /s\\s\n",
    "    if id_word_dict.get(None):\n",
    "      del id_word_dict[None]\n",
    "\n",
    "    return id_word_dict, result\n",
    "\n",
    "#word pieces embedding combination based on tokens ids\n",
    "def combine_subembeddings(ids, embeddings, device = None):\n",
    "\n",
    "    # Ensure both input lists have the same length\n",
    "    if len(ids) != len(embeddings):\n",
    "        raise ValueError(\"Input lists must have the same length\")\n",
    "\n",
    "    # Create a dictionary to store embedding of word pieces by id\n",
    "    id_emb_dict = {}\n",
    "\n",
    "    # Iterate through the lists and populate the dictionary\n",
    "    for id, sub_emb in zip(ids, embeddings):\n",
    "        if id not in id_emb_dict:\n",
    "            id_emb_dict[id] = []\n",
    "        if device:\n",
    "          id_emb_dict[id].append(sub_emb.cpu().numpy().astype(float))\n",
    "\n",
    "        else:\n",
    "          id_emb_dict[id].append(sub_emb.numpy().astype(float))\n",
    "\n",
    "    # Create the list of tuples by averaging embedding pieces\n",
    "    result = [(id, np.mean(sub_emb, axis = 0)) for id, sub_emb in id_emb_dict.items() if not id == None]\n",
    "\n",
    "    #get rid of None key if any. If present is for the special tokens /s\\s\n",
    "    if id_emb_dict.get(None):\n",
    "      del id_emb_dict[None]\n",
    "\n",
    "    return id_emb_dict, result\n",
    "\n",
    "# helper function to extract representations with a given model\n",
    "def feature_extractor(sent, token, tokenizer, model, device = None):    #token\n",
    "    tokenized_sent = tokenizer(sent, return_tensors = \"pt\", truncation = True)\n",
    "    word_ids = tokenized_sent.word_ids()\n",
    "    #dynamically get the target token id\n",
    "    _, combined_words = combine_subwords(word_ids, tokenized_sent.tokens())\n",
    "    combined_words = [i[1] for i in combined_words]\n",
    "    #ensure to get both lower cased and non-lower cased tokens (different between tokenizers)\n",
    "    try:\n",
    "      tokid = combined_words.index(token.lower())\n",
    "    except:\n",
    "      tokid = combined_words.index(token)\n",
    "    \n",
    "    #insert code for the gpu\n",
    "    if device:\n",
    "      with torch.no_grad():\n",
    "        output = model(**tokenized_sent.to(device))\n",
    "    else:\n",
    "      with torch.no_grad():\n",
    "          output = model(**tokenized_sent)\n",
    "    embeddings = output[\"last_hidden_state\"][0,:]\n",
    "    embs_dict, encoded_sent_fw = combine_subembeddings(word_ids, embeddings, device = device)\n",
    "    \n",
    "    return  embs_dict, encoded_sent_fw, tokid\n",
    "\n",
    "#helper function to select the target embeddings\n",
    "def extract_target_embs(encoded_sent_fw, tokid, embs_dict):\n",
    "    target= encoded_sent_fw[tokid][1]\n",
    "    target_sub_embs = embs_dict[tokid]\n",
    "\n",
    "    return target, target_sub_embs\n",
    "\n",
    "#main function to loop over all the sentences and get the target representations\n",
    "def get_target_embeddings(sents, tokens, sent_ids, lemmas, tokenizer, model, device = None):  \n",
    "    if device:\n",
    "      device = device\n",
    "    target_embeddings = {}\n",
    "    total_sub_embs = {}\n",
    "    #loop over the sentences to extract each representation\n",
    "    for i in tqdm(range(len(sents))):\n",
    "        \n",
    "        sent_id = str(sent_ids[i])\n",
    "        token = tokens[i]\n",
    "        sent = sents[i]\n",
    "        lemma = lemmas[i]\n",
    "        #extract the features for the whole sentence\n",
    "        embs_dict, encoded_sent_fw, target_tokid = feature_extractor(sent, token, tokenizer, model, device =device)   #token\n",
    "    \n",
    "        #extract the target embeddings from the given sentence\n",
    "        target, target_sub_embs = extract_target_embs(encoded_sent_fw, target_tokid, embs_dict)\n",
    "        #join the token and sent id to create a key for the dict\n",
    "        key = lemma +\".\"+sent_id\n",
    "        #add value to the key\n",
    "        target_embeddings[key] = target\n",
    "        #store the sub embs in a dictionary with k=word.semt_id: [e1...en]\n",
    "        total_sub_embs[key] = target_sub_embs\n",
    "  \n",
    "    return target_embeddings, total_sub_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper func to write the results\n",
    "def serialize_embs(embs, file_name:str, model_ckp:str, output_path = \"\"):\n",
    "    #write the output in the dedicated directory\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "\n",
    "    if \"/\" in model_ckp:\n",
    "        model_ckp = model_ckp.split(\"/\")[-1]\n",
    "\n",
    "    with open(os.path.join(output_path, file_name)+model_ckp.split(\"/\")[-1]+\".pkl\", \"wb\") as outfile:\n",
    "        pickle.dump(embs, outfile)\n",
    "    return print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the necessary data and metadata\n",
    "full_sents = anticausative_df[\"Sent.\"].tolist()\n",
    "tokens = anticausative_df[\"Token.Form\"].tolist()\n",
    "sent_ids = anticausative_df.index.tolist()\n",
    "lemmas = anticausative_df[\"Lemma\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BabyBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizerFast\n",
    "#Model initialization\n",
    "model_ckp = \"phueb/BabyBERTa-2\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_ckp, add_prefix_space = True, truncation = True, max_length = 130)\n",
    "model = RobertaModel.from_pretrained(model_ckp)\n",
    "#set the model max_length\n",
    "tokenizer.model_max_length= 128\n",
    "\n",
    "#extract the embeddings for each word, by averaging when needed and\n",
    "#separately store the sub-word embeddings for each token\n",
    "target_embeddings, total_sub_embs = get_target_embeddings(full_sents, tokens, sent_ids,lemmas,tokenizer,model)\n",
    "\n",
    "#serialize the target embeddings\n",
    "serialize_embs(target_embeddings, \"target_AC_embeddings_\", model_ckp, output_path = \"Extracted_Embeddings_AC\")\n",
    "serialize_embs(total_sub_embs, \"total_AC_subembs_\",model_ckp, output_path = \"Extracted_Embeddings_AC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythia 70m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "model_ckp = \"EleutherAI/pythia-70m-deduped\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_ckp, revision=\"step3000\", cache_dir=\"./pythia-70m-deduped/step3000\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckp, revision=\"step3000\", cache_dir=\"./pythia-70m-deduped/step3000\")\n",
    "\n",
    "#extract the embeddings for each word, by averaging when needed and\n",
    "#separately store the sub-word embeddings for each token\n",
    "target_embeddings, total_sub_embs = get_target_embeddings(full_sents, tokens, sent_ids,lemmas,tokenizer,model)\n",
    "\n",
    "#serialize the target embeddings\n",
    "serialize_embs(target_embeddings, \"target_AC_embeddings_\", model_ckp, output_path = \"Extracted_Embeddings_AC\")\n",
    "serialize_embs(total_sub_embs, \"total_AC_subembs_\",model_ckp, output_path = \"Extracted_Embeddings_AC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2Model\n",
    "model_ckp = 'gpt2-xl'\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_ckp, return_tensors = \"pt\")\n",
    "model = GPT2Model.from_pretrained(model_ckp, device_map = \"auto\")\n",
    "target_embeddings, total_sub_embs = get_target_embeddings(full_sents, tokens, sent_ids,lemmas,tokenizer,model, device = device)\n",
    "\n",
    "#serialize the target embeddings\n",
    "serialize_embs(target_embeddings, \"target_AC_embeddings_\", model_ckp, output_path = \"Extracted_Embeddings_AC\")\n",
    "serialize_embs(total_sub_embs, \"total_AC_subembs_\",model_ckp, output_path = \"Extracted_Embeddings_AC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "#Model initialization\n",
    "model_ckp = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_ckp)\n",
    "model = DistilBertModel.from_pretrained(model_ckp) #.to(device)\n",
    "\n",
    "#extract the embeddings for each word, by averaging when needed and\n",
    "#separately store the sub-word embeddings for each token\n",
    "target_embeddings, total_sub_embs = get_target_embeddings(full_sents, tokens,sent_ids,lemmas,tokenizer,model) # ,device =device)\n",
    "\n",
    "# #serialize the target embeddings\n",
    "#serialize the target embeddings\n",
    "serialize_embs(target_embeddings, \"target_AC_embeddings_\", model_ckp, output_path = \"Extracted_Embeddings_AC\")\n",
    "serialize_embs(total_sub_embs, \"total_AC_subembs_\",model_ckp, output_path = \"Extracted_Embeddings_AC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperLeL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
