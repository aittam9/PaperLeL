{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ok9Y9OvZlmNH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BywPd_lllyPs"
      },
      "outputs": [],
      "source": [
        "assert torch.cuda.is_available()\n",
        "device = \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q2_cdiIJl7QI"
      },
      "outputs": [],
      "source": [
        "#word pieces combination based on tokens ids\n",
        "def combine_subwords(ids, words):\n",
        "    \n",
        "    \"\"\"The function takes in two arguments both obtained with hf TokenizerFast class:\\\n",
        "    ids: a list of successive non-single ids \\\n",
        "    word_pieces: a list of word pieces\n",
        "    \n",
        "    return:\n",
        "    a dictionary mapping the ids to their respective subwords\n",
        "    a list of the reconstructed words\"\"\"\n",
        "    \n",
        "    words = list(map(lambda x:x.replace(\"Ġ\", \"\").strip(), words))\n",
        "    \n",
        "    # Ensure both input lists have the same length\n",
        "    if len(ids) != len(words):\n",
        "        raise ValueError(\"Input lists must have the same length\")\n",
        "\n",
        "    # Create a dictionary to store word pieces by id\n",
        "    id_word_dict = {}\n",
        "\n",
        "    # Iterate through the lists and populate the dictionary\n",
        "    for id, word in zip(ids, words):\n",
        "        if id not in id_word_dict:\n",
        "            id_word_dict[id] = []\n",
        "        id_word_dict[id].append(word)\n",
        "\n",
        "    # Create the list of tuples by joining the word pieces\n",
        "    result = [(id, ''.join(word_pieces)) for id, word_pieces in id_word_dict.items() if not id == None]\n",
        "\n",
        "    #get rid of None key if any. If present is for the special tokens /s\\s\n",
        "    if id_word_dict.get(None):\n",
        "      del id_word_dict[None]\n",
        "\n",
        "    return id_word_dict, result\n",
        "\n",
        "#word pieces combination based on tokens ids\n",
        "def combine_subembeddings(ids, embeddings, device = None):\n",
        "\n",
        "    # Ensure both input lists have the same length\n",
        "    if len(ids) != len(embeddings):\n",
        "        raise ValueError(\"Input lists must have the same length\")\n",
        "\n",
        "    # Create a dictionary to store embedding of word pieces by id\n",
        "    id_emb_dict = {}\n",
        "\n",
        "    # Iterate through the lists and populate the dictionary\n",
        "    for id, sub_emb in zip(ids, embeddings):\n",
        "        if id not in id_emb_dict:\n",
        "            id_emb_dict[id] = []\n",
        "        if device:\n",
        "          id_emb_dict[id].append(sub_emb.cpu().numpy().astype(float))\n",
        "\n",
        "        else:\n",
        "          id_emb_dict[id].append(sub_emb.numpy().astype(float))\n",
        "\n",
        "    # Create the list of tuples by averaging embedding pieces\n",
        "    result = [(id, np.mean(sub_emb, axis = 0)) for id, sub_emb in id_emb_dict.items() if not id == None]\n",
        "\n",
        "    #get rid of None key if any. If present is for the special tokens /s\\s\n",
        "    if id_emb_dict.get(None):\n",
        "      del id_emb_dict[None]\n",
        "\n",
        "    return id_emb_dict, result\n",
        "\n",
        "# helper function to extract representations with a given model\n",
        "def feature_extractor(sent, token, tokenizer, model, device = None):    #token\n",
        "    tokenized_sent = tokenizer(sent, return_tensors = \"pt\", truncation = True)\n",
        "    word_ids = tokenized_sent.word_ids()\n",
        "    #dynamically get the target token id\n",
        "    _, combined_words = combine_subwords(word_ids, tokenized_sent.tokens())\n",
        "    combined_words = [i[1] for i in combined_words]\n",
        "    #ensure to get both lower cased and non-lower cased tokens (different between tokenizers)\n",
        "    try:\n",
        "      tokid = combined_words.index(token.lower())\n",
        "    except:\n",
        "      tokid = combined_words.index(token)\n",
        "    \n",
        "    #insert code for the gpu\n",
        "    if device:\n",
        "      with torch.no_grad():\n",
        "        output = model(**tokenized_sent.to(device))\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "          output = model(**tokenized_sent)\n",
        "    embeddings = output[\"last_hidden_state\"][0,:]\n",
        "    embs_dict, encoded_sent_fw = combine_subembeddings(word_ids, embeddings, device = device)\n",
        "    \n",
        "    return  embs_dict, encoded_sent_fw, tokid\n",
        "\n",
        "#helper function to select the target embeddings\n",
        "def extract_target_embs(encoded_sent_fw, tokid, embs_dict):\n",
        "    target= encoded_sent_fw[tokid][1]\n",
        "    target_sub_embs = embs_dict[tokid]\n",
        "\n",
        "    return target, target_sub_embs\n",
        "\n",
        "#main function to loop over all the sentences and get the target representations\n",
        "def get_target_embeddings(sents, tokens, sent_ids, tokenizer, model, device = None):  \n",
        "    if device:\n",
        "      device = device\n",
        "    target_embeddings = {}\n",
        "    total_sub_embs = {}\n",
        "    #loop over the sentences to extract each representation\n",
        "    for i in tqdm(range(len(sents))):\n",
        "        \n",
        "        sent_id = str(sent_ids[i])\n",
        "        token = tokens[i]\n",
        "        sent = sents[i]\n",
        "        \n",
        "        #extract the features for the whole sentence\n",
        "        embs_dict, encoded_sent_fw, target_tokid = feature_extractor(sent, token, tokenizer, model, device =device)   #token\n",
        "    \n",
        "        #extract the target embeddings from the given sentence\n",
        "        target, target_sub_embs = extract_target_embs(encoded_sent_fw, target_tokid, embs_dict)\n",
        "        #join the token and sent id to create a key for the dict\n",
        "        key = token +\".\"+sent_id\n",
        "        #add value to the key\n",
        "        target_embeddings[key] = target\n",
        "        #store the sub embs in a dictionary with k=word.semt_id: [e1...en]\n",
        "        total_sub_embs[key] = target_sub_embs\n",
        "  \n",
        "    return target_embeddings, total_sub_embs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ATrd3iJgnFRB"
      },
      "outputs": [],
      "source": [
        "#helper func to write the results\n",
        "def serialize_embs(embs, file_name:str, model_ckp:str):\n",
        "    #write the output in the dedicated directory\n",
        "    output_path = \"..\\\\Data\\\\Extracted_Embeddings\"\n",
        "\n",
        "    if \"/\" in model_ckp:\n",
        "        model_ckp = model_ckp.split(\"/\")[-1]\n",
        "\n",
        "    with open(os.path.join(output_path, file_name)+model_ckp.split(\"/\")[-1]+\".pkl\", \"wb\") as outfile:\n",
        "        pickle.dump(embs, outfile)\n",
        "    return print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tLwu-t1xl3o8"
      },
      "outputs": [],
      "source": [
        "with open(\"Data/complete_df.pkl\", \"rb\") as infile:\n",
        "    complete_df = pickle.load(infile)\n",
        "#extract the necessary data and metadata\n",
        "full_sents = complete_df[\"Sent._x\"].tolist()\n",
        "ids2extract = complete_df[\"hf_tnzd_ids\"].tolist()\n",
        "tokens = complete_df[\"Token\"].tolist()\n",
        "sent_ids = complete_df.index.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWENXL35mNoo"
      },
      "source": [
        "## BabyBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtvqKqXkl6tc",
        "outputId": "0a1a78f8-fea7-4353-a0e9-b12a5f5a4a25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at phueb/BabyBERTa-2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 1955/1955 [00:47<00:00, 41.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n",
            "Done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaModel, RobertaTokenizerFast\n",
        "#Model initialization\n",
        "model_ckp = \"phueb/BabyBERTa-2\"\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(model_ckp, add_prefix_space = True, truncation = True, max_length = 130)\n",
        "model = RobertaModel.from_pretrained(model_ckp)\n",
        "#set the model max_length\n",
        "tokenizer.model_max_length= 128\n",
        "\n",
        "#extract the embeddings for each word, by averaging when needed and\n",
        "#separately store the sub-word embeddings for each token\n",
        "target_embeddings, total_sub_embs = get_target_embeddings(full_sents, tokens, sent_ids,tokenizer,model)\n",
        "\n",
        "#serialize the target embeddings\n",
        "serialize_embs(target_embeddings, \"target_embeddings_\", model_ckp)\n",
        "serialize_embs(total_sub_embs, \"total_subembs_\",model_ckp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B339XYthmoOD"
      },
      "source": [
        "## GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdtScXhlmdHn"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2TokenizerFast, GPT2Model\n",
        "model_ckp = 'gpt2-large'\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_ckp, return_tensors = \"pt\")\n",
        "model = GPT2Model.from_pretrained(model_ckp, device_map = \"auto\")\n",
        "target_embeddings, total_sub_embs = get_target_embeddings(full_sents, tokens, sent_ids,tokenizer,model, device = device)\n",
        "\n",
        "#serialize the target embeddings\n",
        "serialize_embs(target_embeddings, \"target_embeddings_\", model_ckp)\n",
        "serialize_embs(total_sub_embs, \"total_subembs_\",model_ckp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0JaWxG4mq9U"
      },
      "source": [
        "## Pythia 70M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eorV3Lc1mqpc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "model_ckp = \"EleutherAI/pythia-70m-deduped\"\n",
        "\n",
        "model = AutoModel.from_pretrained(model_ckp, revision=\"step3000\", cache_dir=\"./pythia-1b-deduped/step3000\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckp, revision=\"step3000\", cache_dir=\"./pythia-70m-deduped/step3000\")\n",
        "\n",
        "#extract the embeddings for each word, by averaging when needed and\n",
        "#separately store the sub-word embeddings for each token\n",
        "target_embeddings, total_sub_embs = get_target_embeddings(full_sents, tokens, sent_ids,tokenizer,model)\n",
        "\n",
        "#serialize the extracted embeddings\n",
        "serialize_embs(target_embeddings, \"target_embeddings_\", model_ckp)\n",
        "serialize_embs(total_sub_embs, \"total_subembs_\",model_ckp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CniCrA3psLaS"
      },
      "source": [
        "## GPT-2 XL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovZUyDEmsDAU"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2TokenizerFast, GPT2Model\n",
        "model_ckp = 'gpt2-xl'\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_ckp, return_tensors = \"pt\")\n",
        "model = GPT2Model.from_pretrained(model_ckp, device_map = \"auto\")\n",
        "target_embeddings, total_sub_embs = get_target_embeddings(full_sents, tokens, sent_ids,tokenizer,model, device = device)\n",
        "\n",
        "#serialize the target embeddings\n",
        "serialize_embs(target_embeddings, \"target_embeddings_\", model_ckp)\n",
        "serialize_embs(total_sub_embs, \"total_subembs_\",model_ckp)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
