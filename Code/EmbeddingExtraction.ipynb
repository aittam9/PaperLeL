{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from transformers import RobertaModel, RobertaTokenizerFast\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..\\\\Data\\\\complete_df.pkl\", \"rb\") as infile:\n",
    "    complete_df = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sent._x</th>\n",
       "      <th>Split</th>\n",
       "      <th>Pred.Token</th>\n",
       "      <th>Pred.Lemma</th>\n",
       "      <th>Sent._y</th>\n",
       "      <th>hf_tknzd_sents</th>\n",
       "      <th>Token</th>\n",
       "      <th>hf_tnzd_ids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The police commander of Ninevah Province annou...</td>\n",
       "      <td>train</td>\n",
       "      <td>6</td>\n",
       "      <td>announce</td>\n",
       "      <td>[The, police, commander, of, Ninevah, Province...</td>\n",
       "      <td>[the, police, commander, of, ninevah, province...</td>\n",
       "      <td>announced</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Guerrillas killed an engineer, Asi Ali, from T...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>kill</td>\n",
       "      <td>[Guerrillas, killed, an, engineer, ,, Asi, Ali...</td>\n",
       "      <td>[guerrillas, killed, an, engineer, ,, asi, ali...</td>\n",
       "      <td>killed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Guerrillas near Hawijah launched an attack tha...</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "      <td>launch</td>\n",
       "      <td>[Guerrillas, near, Hawijah, launched, an, atta...</td>\n",
       "      <td>[guerrillas, near, hawijah, launched, an, atta...</td>\n",
       "      <td>launched</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sent._x  Split  Pred.Token  \\\n",
       "Idx                                                                         \n",
       "12   The police commander of Ninevah Province annou...  train           6   \n",
       "18   Guerrillas killed an engineer, Asi Ali, from T...  train           1   \n",
       "22   Guerrillas near Hawijah launched an attack tha...  train           3   \n",
       "\n",
       "    Pred.Lemma                                            Sent._y  \\\n",
       "Idx                                                                 \n",
       "12    announce  [The, police, commander, of, Ninevah, Province...   \n",
       "18        kill  [Guerrillas, killed, an, engineer, ,, Asi, Ali...   \n",
       "22      launch  [Guerrillas, near, Hawijah, launched, an, atta...   \n",
       "\n",
       "                                        hf_tknzd_sents      Token  hf_tnzd_ids  \n",
       "Idx                                                                             \n",
       "12   [the, police, commander, of, ninevah, province...  announced            6  \n",
       "18   [guerrillas, killed, an, engineer, ,, asi, ali...     killed            1  \n",
       "22   [guerrillas, near, hawijah, launched, an, atta...   launched            3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the necessary data and metadata\n",
    "full_sents = complete_df[\"Sent._x\"].tolist()\n",
    "ids2extract = complete_df[\"hf_tnzd_ids\"].tolist()\n",
    "tokens = complete_df[\"Token\"].tolist()\n",
    "sent_ids = complete_df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word pieces combination based on tokens ids\n",
    "def combine_subembeddings(ids, embeddings):\n",
    "\n",
    "    # Ensure both input lists have the same length\n",
    "    if len(ids) != len(embeddings):\n",
    "        raise ValueError(\"Input lists must have the same length\")\n",
    "\n",
    "    # Create a dictionary to store embedding of word pieces by id\n",
    "    id_emb_dict = {}\n",
    "\n",
    "    # Iterate through the lists and populate the dictionary\n",
    "    for id, sub_emb in zip(ids, embeddings):\n",
    "        if id not in id_emb_dict:\n",
    "            id_emb_dict[id] = []\n",
    "        id_emb_dict[id].append(sub_emb.numpy().astype(float))\n",
    "\n",
    "    # Create the list of tuples by joining the word pieces\n",
    "    result = [(id, np.mean(sub_emb, axis = 0)) for id, sub_emb in id_emb_dict.items() if not id == None]\n",
    "\n",
    "    #get rid of None key if any. If present is for the special tokens /s\\s\n",
    "    if id_emb_dict.get(None):\n",
    "      del id_emb_dict[None]\n",
    "\n",
    "    return id_emb_dict, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to extract representations with a given model\n",
    "def feature_extractor(sent, tokenizer, model):\n",
    "    tokenized_sent = tokenizer(sent, return_tensors = \"pt\")\n",
    "    word_ids = tokenized_sent.word_ids()\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokenized_sent)\n",
    "    embeddings = output[\"last_hidden_state\"][0,:]\n",
    "    embs_dict, encoded_sent_fw = combine_subembeddings(word_ids, embeddings)\n",
    "    \n",
    "    return  embs_dict, encoded_sent_fw\n",
    "\n",
    "#helper function to select the target embeddings\n",
    "def extract_target_embs(encoded_sent_fw, tokid, embs_dict):\n",
    "    target= encoded_sent_fw[tokid][1]\n",
    "    target_sub_embs = embs_dict[tokid]\n",
    "\n",
    "    return target, target_sub_embs\n",
    "\n",
    "#main function to loop over all the sentences and get the target representations\n",
    "def get_target_embeddings(sents, tokens, sent_ids, ids2extract, tokenizer, model):  \n",
    "    \n",
    "    target_embeddings = {}\n",
    "    total_sub_embs = {}\n",
    "    #loop over the sentences to extract each representation\n",
    "    for i in tqdm(range(len(sents))):\n",
    "        try:\n",
    "            sent_id = str(sent_ids[i])\n",
    "            token = tokens[i]\n",
    "            sent = sents[i]\n",
    "            target_tokid = ids2extract[i]\n",
    "        \n",
    "            try:\n",
    "                #extract the features for the whole sentence\n",
    "                embs_dict, encoded_sent_fw = feature_extractor(sent, tokenizer, model)   \n",
    "            except:\n",
    "                print(f\"Tensor problem for sent {sent_id}\")\n",
    "                pass\n",
    "            #extract the target embeddings from the given sentence\n",
    "            target, target_sub_embs = extract_target_embs(encoded_sent_fw, target_tokid, embs_dict)\n",
    "            #join the token and sent id to create a key for the dict\n",
    "            key = token +\".\"+sent_id\n",
    "            #add value to the key\n",
    "            target_embeddings[key] = target\n",
    "            #store \n",
    "            \n",
    "            total_sub_embs[key] = target_sub_embs\n",
    "        except:\n",
    "            print(f\"something went wrong for sent {sent_id}\")\n",
    "            pass\n",
    "        \n",
    "    return target_embeddings, total_sub_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baby Berta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at phueb/BabyBERTa-2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 52%|█████▏    | 1012/1955 [00:25<00:26, 35.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor problem for sent 6780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1955/1955 [00:53<00:00, 36.83it/s]\n"
     ]
    }
   ],
   "source": [
    "#Model initialization\n",
    "model_ckp = \"phueb/BabyBERTa-2\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_ckp, add_prefix_space = True, truncation = True)\n",
    "model = RobertaModel.from_pretrained(model_ckp)\n",
    "\n",
    "#extract the embeddings for each word, by averaging when needed and\n",
    "#separately store the sub-word embeddings for each token\n",
    "target_embeddings, total_sub_embs = get_target_embeddings(full_sents, tokens, sent_ids,ids2extract,tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['announced.12', 'killed.18', 'launched.22', 'said.27', 'reports.38', 'managed.39', 'were.46', 'is.48', 'are.52', 'guess.56', 'said.63', 'mentioned.64', 'added.65', 'know.67', 'is.74', 'published.78', 'continues.81', 'is.85', 'are.87', 'present.89', 'had.118', 'offers.122', 'have.125', 'are.132', 'held.135', 'attempted.145', 'generates.146', 'owes.153', 'have.159', 'need.160', 'is.163', 'alleged.164', 'is.165', 'fails.167', 'faltered.172', 'asked.174', 'asked.178', 'asked.179', 'replied.180', 'released.183', 'said.184', 'said.187', 'seems.188', 'is.200', 'is.202', 'had.223', 'created.226', 'chose.228', 'publishes.248', 'feel.249', 'are.250', 'explains.257', 'said.259', 'told.261', 'held.262', 'preceded.264', 'said.265', 'lay.268', 'said.269', 'said.270', 'told.271', 'had.276', 'failed.277', 'complained.279', 'added.284', 'pointed.285', 'calls.293', 'is.298', 'understand.300', 'understood.303', 'remains.317', 'is.319', 'know.329', 'flew.332', 'saw.333', 'follows.342', 'joined.353', 'kept.363', 'earned.366', 'earned.367', 'earned.368', 'earned.370', 'earned.371', 'indicate.372', 'brings.374', 'asked.375', 'said.376', 'says.377', 'says.378', 'earned.383', 'accumulated.385', 'received.389', 'said.390', 'called.391', 'called.392', 'blames.397', 'appears.406', 'suspect.408', 'is.409', 'affirmed.415', 'stated.417', 'has.429', 'had.434', 'described.436', 'said.438', 'contend.440', 'said.444', 'concur.445', 'rose.454', 'believe.458', 'thought.465', 'happened.478', 'implicated.480', 'doubt.484', 'expect.485', 'threatens.492', 'identifies.495', 'seem.496', 'includes.499', 'needed.502', 'signed.507', 'states.508', 'threw.510', 'Implicated.515', 'made.517', 'said.519', 'featured.525', 'like.532', 'Is.552', 'came.565', 'found.566', 'travel.571', 'quoted.575', 'said.576', 'suggests.577', 'makes.580', 'make.581', 'sit.584', 'launched.594', 'left.599', 'reported.603', 'maintained.608', 'indicate.618', 'passes.619', 'comprise.621', 'divides.623', 'writes.626', 'claim.630', 'says.631', 'deny.632', 'says.633', 'told.639', 'said.644', 'says.649', 'said.650', 'added.651', 'says.653', 'say.657', 'says.659', 'adds.660', 'remains.663', 'welcomed.673', 'says.680', 'made.683', 'feared.684', 'say.691', 'fear.692', 'say.693', 'says.695', 'says.698', 'warned.701', 'voted.702', 'did.716', 'gave.721', 'reported.724', 'has.726', 'suffers.734', 'know.735', 'gives.738', 'scoured.741', 'is.742', 'has.744', 'led.757', 'became.758', 'was.759', 'were.762', 'says.764', 'says.766', 'was.771', 'dated.779', 'left.781', 'says.782', 'thought.788', 'thought.789', 'was.808', 'follows.813', 'said.814', 'have.817', 'confirmed.818', 'said.820', 'are.821', 'claimed.822', 'occurred.832', 'said.833', 'said.835', 'left.836', 'turned.837', 'have.847', 'said.862', 'acknowledged.867', \"'s.870\", 'count.877', 'is.884', 'marry.887', 'begs.890', 'suppose.903', 'say.905', 'deserves.907', 'have.910', 'told.916', 'said.917', 'saw.919', 'wanted.920', 'has.923', 'cited.925', 'be.938', 'hates.945', 'agree.947', 'detect.954', 'think.956', 'have.958', 'trust.974', 'led.980', 'disappeared.981', 'refused.984', 'began.985', 'became.990', 'says.992', 'think.993', 'know.1000', 'want.1005', 'need.1006', 'trust.1017', 'is.1029', 'died.1035', 'blended.1036', 'means.1038', 'am.1042', 'resisted.1050', 'like.1056', 'boggles.1090', 'seem.1091', 'have.1098', 'guess.1101', 'prefer.1113', 'refer.1114', 'is.1117', 'had.1118', 'has.1119', 'have.1120', 'had.1124', 'had.1125', 'is.1126', 'have.1127', 'have.1128', 'have.1129', 'is.1140', 'become.1141', 'die.1153', 'killed.1154', 'creates.1156', 'comes.1162', 'has.1176', 'promise.1183', 'are.1187', 'kill.1207', 'believe.1210', 'remember.1211', 'recommend.1224', 'walks.1229', 'condemn.1232', 'is.1233', 'are.1234', 'provide.1236', 'funds.1242', 'operates.1247', 'is.1248', 'meet.1258', 'believes.1262', 'are.1263', 'debates.1265', 'leave.1286', 'is.1287', 'is.1292', 'is.1293', 'need.1304', 'has.1322', 'play.1326', 'sponsors.1327', 'have.1347', 'is.1354', 'were.1355', 'included.1359', 'concluded.1361', 'concluded.1362', 'included.1365', 'noted.1366', 'says.1372', 'included.1374', 'is.1378', 'argue.1380', 'explained.1384', 'indicate.1394', 'believed.1395', 'reported.1397', 'urged.1405', 'said.1406', 'thought.1408', 'threatened.1411', 'testified.1414', 'explained.1415', 'testified.1416', 'admitted.1420', 'said.1422', 'said.1423', 'was.1425', 'noted.1427', 'is.1430', 'was.1431', 'point.1434', 'undermines.1437', 'had.1441', 'met.1442', 'has.1444', 'used.1445', 'symbolizes.1446', 'provided.1448', 'were.1449', 'traveled.1450', 'reported.1454', 'reported.1460', 'was.1469', 'say.1472', 'advises.1473', 'advises.1477', 'reports.1480', 'concluded.1485', 'appears.1487', 'show.1495', 'feel.1502', 'expressed.1503', 'remains.1506', 'pale.1509', 'had.1510', 'argued.1511', 'includes.1515', 'explained.1516', 'is.1518', 'supports.1519', 'recalls.1521', 'sent.1523', 'was.1527', 'was.1529', 'was.1530', 'led.1536', 'sent.1546', 'explained.1552', 'establish.1554', 'symbolizes.1555', 'means.1557', 'explains.1561', 'traveled.1565', 'deserve.1567', 'hope.1590', 'have.1613', 'do.1641', 'suggest.1642', 'appreciate.1649', 'believe.1676', 'noticed.1678', 'cares.1697', 'invited.1701', 'is.1706', 'is.1716', 'understand.1738', 'know.1740', 'communicated.1744', 'need.1777', 'think.1783', 'have.1805', 'has.1808', 'is.1809', 'appears.1811', 'has.1812', 'think.1818', 'think.1824', 'see.1845', 'found.1864', 'is.1872', 'listed.1873', 'want.1875', 'realize.1876', 'drool.1900', 'rule.1903', 'rule.1907', 'see.1913', 'thought.1923', 'stayed.1939', 'goes.1942', 'has.1948', 'means.1950', 'refer.1956', 'have.1959', 'see.1966', 'says.1977', 'expect.1988', 'hear.1998', 'puts.2006', 'came.2007', 'says.2010', 'has.2011', 'looks.2012', 'told.2026', 'told.2027', 'said.2028', 'think.2039', 'called.2045', 'think.2064', 'says.2083', 'sounds.2094', 'believe.2096', 'said.2103', 'has.2104', 'think.2109', 'think.2113', 'was.2118', 'is.2122', 'noticed.2133', 'did.2150', 'remain.2154', 'means.2168', 'is.2174', 'means.2175', 'is.2176', 'issued.2213', 'have.2225', 'reflects.2226', 'says.2234', 'talked.2254', 'said.2287', 'bet.2307', 'looks.2314', 'review.2317', 'informed.2324', 'says.2326', 'did.2337', 'approached.2340', 'said.2342', 'approved.2343', 'KNOW.2351', 'HAVE.2354', 'asked.2367', 'received.2368', 'thought.2369', 'sold.2408', 'works.2411', 'reflects.2480', 'think.2491', 'wanted.2513', 'need.2542', 'need.2555', 'have.2565', 'hope.2570', 'showed.2572', 'hope.2602', 'received.2620', 'want.2663', 'enjoyed.2668', 'hope.2688', 'want.2695', 'are.2702', 'said.2712', 'hope.2726', 'believe.2746', 'believe.2765', 'gave.2767', 'have.2770', 'said.2771', 'said.2772', 'spoke.2779', 'need.2780', 'said.2783', 'used.2790', 'think.2796', 'said.2799', 'has.2806', 'are.2813', 'understand.2820', 'need.2822', 'found.2823', 'thought.2829', 'say.2832', 'believe.2837', 'assume.2844', 'sent.2846', 'need.2848', 'need.2856', 'know.2872', 'know.2890', 'spoke.2897', 'was.2898', 'is.2902', 'work.2927', 'played.2992', 'had.2995', 'expect.2996', 'think.2997', 'guess.3002', 'got.3005', 'guess.3008', 'leave.3010', 'did.3018', 'said.3019', 'think.3020', 'is.3058', 'have.3062', 'spoke.3074', 'figured.3082', 'had.3084', 'told.3098', 'think.3102', 'told.3118', 'forgot.3130', 'ran.3132', 'think.3136', 'know.3148', 'is.3151', 'is.3165', 'think.3172', 'phoned.3184', 'was.3186', 'think.3194', 'intend.3206', 'intend.3215', 'wish.3216', 'believe.3253', 'left.3268', 'looked.3269', 'know.3304', 'received.3344', 'know.3364', 'was.3378', 'believe.3382', 'propose.3433', 'reflects.3440', 'need.3462', 'indicated.3483', 'await.3485', 'appreciated.3491', 'are.3503', 'is.3541', 'gets.3552', 'sez.3555', 'asked.3560', 'hope.3565', 'believe.3575', 'include.3594', 'amends.3620', 'communicated.3627', 'adds.3644', 'was.3677', 'exists.3686', 'intend.3727', 'understand.3734', 'understand.3748', 'understand.3763', 'know.3770', 'want.3785', 'enables.3801', 'obligates.3805', 'need.3808', 'sounds.3812', 'was.3818', 'did.3824', 'is.3829', 'propose.3832', 'thought.3836', 'is.3838', 'incorporates.3847', 'made.3867', 'left.3877', 'believe.3891', 'forwarded.3892', 'says.3930', 'met.3957', 'made.3959', 'is.3963', 'needs.3977', 'think.3981', 'envision.3989', 'remain.3991', 'met.3999', 'enjoyed.4005', 'have.4006', 'indicated.4010', 'figured.4040', 'looks.4060', 'is.4069', 'expect.4100', 'is.4139', \"'s.4144\", 'refers.4165', 'informed.4192', 'lies.4197', 'is.4201', 'is.4218', 'defeats.4220', 'shows.4253', 'got.4261', 'have.4298', 'think.4299', 'have.4314', 'spoke.4351', 'have.4356', 'discussed.4362', 'wanted.4370', 'suggested.4374', 'understand.4390', 'needs.4391', 'need.4392', 'think.4393', 'understand.4402', 'assume.4411', 'have.4420', 'exist.4484', 'faxed.4500', 'asked.4504', 'know.4505', 'is.4508', 'lead.4522', 'tells.4536', 'assume.4548', 'believe.4578', 'Maintains.4579', 'include.4592', 'extend.4596', 'is.4600', 'begin.4613', 'pose.4643', 'renewed.4644', 'asked.4645', 'joined.4648', 'lauded.4650', 'subsides.4654', 'find.4656', 'estimate.4700', 'are.4702', 'is.4707', 'highlight.4734', 'sounds.4742', 'had.4753', 'decided.4755', 'called.4757', 'arrived.4759', 'were.4761', 'stayed.4762', 'told.4764', 'notified.4765', 'spoke.4773', 'recognize.4781', 'need.4786', 'shows.4798', 'refers.4823', 'understand.4862', 'think.4865', 'needs.4872', 'need.4880', 'think.4894', 'spoke.4895', 'wanted.4942', 'hope.4949', 'has.4993', 'is.5081', 'apologize.5086', 'allege.5105', 'is.5106', 'show.5107', 'said.5108', 'said.5110', 'said.5111', 'said.5113', 'said.5116', 'had.5117', 'said.5120', 'show.5122', 'read.5124', 'said.5126', 'told.5127', 'got.5128', 'killed.5129', 'began.5132', 'followed.5133', 'rested.5135', 'said.5139', 'said.5141', 'accelerated.5143', 'show.5144', 'believes.5145', 'said.5146', 'said.5149', 'said.5150', 'felt.5152', 'said.5153', 'said.5155', 'did.5156', 'said.5158', 'defended.5159', 'said.5160', 'did.5161', 'believes.5163', 'said.5164', 'said.5165', 'is.5171', 'suggest.5175', 'is.5188', 'have.5222', 'sued.5225', 'have.5302', 'do.5367', 'made.5375', 'has.5397', 'see.5404', 'is.5415', 'said.5418', 'is.5427', 'introduces.5443', 'makes.5445', 'needed.5449', 'is.5456', 'includes.5463', 'encourage.5482', 'comes.5490', 'becomes.5502', 'represent.5534', 'wins.5535', 'used.5538', 'said.5539', 'said.5541', 'said.5544', 'said.5546', 'grabbed.5547', 'said.5548', 'said.5551', 'tripped.5552', 'said.5559', 'comes.5565', 'started.5579', 'got.5583', 'wanted.5589', 'got.5591', 'used.5592', 'kept.5604', 'built.5610', 'represents.5613', 'is.5620', 'has.5635', 'stated.5636', 'have.5645', 'are.5654', 'play.5656', 'extend.5663', 'wish.5667', 'have.5675', 'have.5696', 'know.5701', 'are.5712', 'refers.5717', 'NEEDED.5721', 'came.5727', 'solicit.5729', 'sought.5730', 'remained.5734', 'include.5739', 'propose.5742', 'know.5743', 'assure.5745', 'pray.5748', 'said.5757', 'said.5758', 'plan.5759', 'said.5762', 'said.5764', 'told.5766', 'had.5767', 'said.5774', 'warned.5775', 'said.5776', 'says.5777', 'says.5778', 'appears.5782', 'said.5783', 'said.5792', 'urged.5794', 'suggest.5798', 'hit.5799', 'approved.5818', 'reduces.5827', 'increases.5834', 'hope.5837', 'guess.5849', 'believe.5854', 'find.5859', 'think.5860', 'realize.5864', 'costs.5865', 'is.5877', 'is.5879', 'is.5880', 'is.5892', 'needs.5895', 'hope.5897', 'has.5908', 'know.5910', 'included.5927', 'threatened.5930', 'has.5934', 'overtook.5936', 'protested.5940', 'wanted.5944', 'said.5946', 'said.5947', 'said.5949', 'said.5950', 'said.5953', 'marks.5954', 'has.5960', 'surrounds.5974', 'feel.5975', 'is.5978', 'want.5979', 'want.5991', 'swirls.5994', 'is.6005', 'is.6016', 'is.6019', 'is.6027', 'presents.6028', 'creates.6051', 'feel.6052', 'oppose.6054', 'affirm.6055', 'insist.6056', 'respect.6057', 'affirm.6060', 'recognize.6061', 'commit.6064', 'has.6075', 'has.6077', 'has.6078', 'love.6080', 'love.6081', 'loves.6082', 'loves.6083', 'have.6094', 'is.6095', 'believes.6096', 'becomes.6101', 'has.6119', 'launches.6126', 'keep.6127', 'launched.6131', 'follows.6134', 'vaulted.6135', 'believe.6136', 'remain.6137', 'said.6139', 'said.6141', 'said.6147', 'wish.6148', 'said.6150', 'said.6151', 'believes.6155', 'said.6156', 'said.6157', 'said.6160', 'has.6172', 'is.6174', 'read.6181', 'think.6184', 'is.6199', 'have.6204', 'makes.6205', 'increases.6206', 'hope.6217', 'found.6229', 'is.6230', 'started.6233', 'figured.6235', 'made.6236', 'had.6238', 'promise.6243', 'remains.6255', 'think.6282', 'VERIFIES.6291', 'existed.6325', 'claimed.6326', 'seemed.6331', 'explained.6338', 'told.6340', 'say.6341', 'told.6346', 'provide.6349', 'provide.6351', 'has.6353', 'said.6354', 'cautioned.6355', 'said.6360', 'want.6361', 'appeared.6362', 'vacated.6363', 'said.6364', 'are.6369', 'eat.6370', 'seem.6372', 'seem.6375', 'said.6378', 'said.6379', 'fuss.6383', 'published.6390', 'studies.6391', 'found.6392', 'showed.6396', 'shows.6398', 'concluded.6399', 'looked.6400', 'said.6401', 'told.6402', 'said.6403', 'said.6406', 'said.6409', 'include.6414', 'coincided.6417', 'said.6419', 'found.6422', 'concluded.6424', 'agrees.6425', 'falls.6426', 'agreed.6427', 'thinks.6429', 'said.6430', 'include.6432', 'include.6433', 'examines.6435', 'is.6437', 'tried.6438', 'assumed.6439', 'wrote.6441', 'predict.6453', 'think.6464', 'is.6468', 'have.6485', 'became.6487', 'lived.6492', 'is.6493', 'is.6494', 'refused.6499', 'suspect.6503', 'expect.6505', 'is.6521', 'wanted.6523', 'was.6525', 'was.6528', 'started.6531', 'guess.6534', 'is.6536', 'improve.6556', 'include.6562', \"'s.6610\", 'sees.6623', 'continue.6624', 'comes.6637', 'died.6650', 'conquered.6671', 'visited.6676', 'wrote.6677', 'became.6695', 'beat.6716', 'beats.6717', 'said.6725', 'said.6726', 'said.6727', 'said.6728', 'said.6729', 'seemed.6734', 'was.6744', 'went.6745', 'dragged.6748', 'determined.6751', 'swear.6753', 'addressed.6754', 'believe.6756', 'happened.6761', 'came.6763', 'takes.6766', 'began.6775', 'followed.6776', 'Came.6777', 'acted.6778', 'wanted.6780', 'provided.6784', 'used.6786', 'admitted.6788', 'has.6792', 'made.6796', 'got.6799', 'said.6803', 'refers.6804', 'said.6805', 'got.6807', 'said.6808', 'said.6809', 'said.6810', 'thinks.6814', 'recommend.6821', 'reported.6833', 'quoted.6834', 'have.6839', 'has.6850', 'include.6851', 'continued.6853', 'has.6854', 'is.6855', 'reports.6857', 'put.6858', 'cites.6861', 'holds.6865', 'takes.6871', 'state.6872', 'agreed.6886', 'has.6890', 'had.6892', 'holds.6910', 'give.6911', 'have.6914', 'has.6920', 'cemented.6925', 'is.6928', 'have.6931', 'provided.6953', 'lies.6955', 'threatens.6956', 'says.6958', 'run.6959', 'was.6969', 'pledged.7096', 'assigned.7097', 'have.7191', 'employs.7202', 'do.7203', 'is.7204', 'need.7214', 'got.7222', 'crashes.7226', 'focus.7236', 'know.7240', 'abandoned.7249', 'seems.7253', 'has.7264', 'has.7266', 'is.7270', 'is.7282', 'is.7306', 'found.7311', 'loves.7322', 'is.7326', 'works.7333', 'have.7336', 'took.7343', 'loved.7344', 'agree.7358', 'use.7361', 'emailed.7372', 'love.7380', 'have.7394', 'have.7406', 'know.7410', 'has.7412', 'use.7439', 'know.7455', 'needs.7456', 'want.7458', \"'s.7467\", 'made.7472', 'does.7476', 'is.7482', 'have.7487', 'is.7497', 'makes.7509', 'need.7511', 'love.7530', 'dislike.7538', 'live.7543', 'want.7544', 'want.7552', 'born.7572', 'have.7575', 'need.7581', 'carry.7593', 'have.7595', 'is.7601', 'knows.7607', 'hope.7623', 'are.7630', 'loves.7647', 'found.7660', 'found.7662', \"'s.7671\", 'have.7679', 'use.7702', 'want.7721', 'work.7723', 'know.7725', 'need.7726', 'does.7731', 'do.7735', 'had.7746', 'are.7748', 'do.7754', 'is.7757', 'love.7758', 'have.7765', 'have.7774', 'have.7777', 'want.7810', 'asked.7815', 'lists.7818', 'add.7830', 'see.7843', 'appear.7844', 'have.7856', 'have.7859', 'have.7863', 'have.7867', 'have.7868', 'have.7874', 'remember.7879', 'hope.7885', 'is.7890', 'Is.7897', 'need.7901', 'bought.7902', 'think.7928', 'feel.7947', 'get.7949', 'had.7954', 'seems.7955', 'have.7967', 'knew.7969', 'thought.7972', 'eat.7978', 'need.7993', 'are.8001', 'have.8012', 'thought.8021', 'want.8031', 'LOVE.8039', 'used.8044', 'Have.8053', 'use.8061', 'have.8064', 'live.8073', 'covers.8112', 'ensures.8117', 'is.8130', 'is.8138', \"'s.8139\", 'said.8150', 'are.8151', 'ordered.8154', 'was.8157', 'used.8162', 'uses.8164', 'play.8165', 'have.8183', 'have.8214', 'are.8218', 'speak.8224', 'tend.8225', 'dislike.8230', 'likes.8234', 'used.8237', 'did.8243', 'is.8245', 'IS.8253', 'is.8255', 'is.8265', 'verifies.8266', 'have.8269', 'take.8285', 'lay.8292', 'have.8295', 'have.8301', 'love.8303', 'know.8313', 'want.8315', 'states.8318', 'mean.8350', 'mean.8352', 'mean.8357', 'began.8367', 'means.8370', 'is.8390', 'are.8407', 'have.8411', 'have.8412', 'have.8415', 'dilute.8416', 'have.8424', 'took.8430', 'took.8432', 'need.8436', 'clashes.8442', 'is.8444', 'is.8448', 'is.8459', 'take.8462', 'are.8467', 'heard.8472', 'do.8486', 'need.8489', 'saw.8493', 'had.8500', 'want.8509', 'are.8526', 'had.8545', 'had.8546', 'had.8549', 'feed.8558', 'did.8561', 'said.8566', 'guess.8569', 'has.8585', 'bet.8586', 'melt.8589', 'melts.8590', 'use.8593', 'enjoy.8601', 'wants.8602', 'think.8604', 'has.8619', 'did.8620', 'bought.8643', 'got.8645', 'moving.8665', 'have.8693', 'saw.8697', 'need.8706', 'goes.8707', 'want.8715', 'start.8732', 'have.8761', 'learned.8771', 'is.8772', 'means.8773', 'have.8781', 'have.8783', 'love.8788', 'wanted.8794', 'realize.8799', 'find.8820', 'do.8830', 'had.8836', 'has.8854', 'refuse.8865', 'sailed.8868', 'go.8869', 'traveled.8871', 'loved.8872', 'are.8873', 'felt.8875', 'liked.8876', 'enjoyed.8878', 'felt.8879', 'get.8880', 'had.8882', 'had.8883', 'had.8884', 'tried.8887', 'Have.8900', 'mean.8904', 'are.8910', 'need.8928', 'recommend.8932', 'adopted.8968', 'is.8971', 'have.8979', 'trained.8980', 'trained.8981', 'have.8988', 'are.8989', 'did.9018', 'buy.9034', 'take.9035', 'went.9043', 'had.9044', 'got.9047', 'mentioned.9048', 'are.9049', 'thought.9079', 'took.9080', 'said.9082', 'end.9106', 'need.9114', 'told.9118', 'thinks.9137', 'seemed.9138', 'said.9145', 'gave.9149', 'is.9151', 'are.9159', 'intends.9164', 'seems.9165', \"'s.9196\", 'guess.9202', 'are.9204', 'means.9212', 'have.9214', 'has.9216', 'know.9220', 'handle.9222', 'crawls.9229', 'has.9243', 'have.9248', 'see.9257', 'have.9265', 'did.9266', 'help.9272', 'need.9274', 'knows.9282', 'have.9303', 'is.9304', 'means.9305', 'refused.9310', 'left.9312', 'is.9313', 'agreed.9319', 'thought.9322', 'detected.9325', 'rejected.9334', 'was.9335', 'kept.9340', 'made.9346', 'is.9347', 'have.9351', 'said.9354', 'have.9356', 'need.9366', 'gives.9380', 'gives.9382', 'tell.9386', 'did.9396', 'ended.9397', 'got.9400', 'want.9404', 'love.9408', 'tamed.9410', 'loves.9412', 'hope.9415', 'has.9423', 'have.9428', 'told.9432', 'took.9440', 'have.9454', 'knows.9456', 'worked.9459', 'have.9466', 'graduate.9469', 'works.9481', 'include.9488', 'is.9491', 'used.9506', 'start.9508', 'had.9509', 'have.9514', 'aspire.9523', 'think.9525', 'are.9535', 'indicates.9553', 'is.9566', 'look.9580', 'are.9585', 'are.9593', 'offer.9595', 'offers.9598', 'are.9601', 'have.9603', 'include.9608', 'are.9611', 'liked.9621', 'tend.9623', 'had.9631', 'had.9632', 'are.9633', 'is.9636', 'think.9645', 'told.9660', 'know.9664', 'is.9674', 'think.9697', 'had.9705', 'was.9709', 'have.9720', 'have.9723', 'is.9731', 'elected.9735', 'eats.9737', 'posted.9752', 'know.9769', 'know.9771', 'think.9779', 'hauled.9789', 'returned.9790', 'is.9796', 'does.9799', 'know.9801', 'believe.9802', 'hate.9806', 'did.9822', 'fixed.9825', 'knows.9828', 'did.9831', 'had.9842', 'found.9843', 'dropped.9846', 'found.9852', 'enjoyed.9853', 'fixed.9856', 'came.9857', 'took.9862', 'like.9874', 'went.9879', 'love.9883', 'attend.9884', 'rang.9889', 'went.9909', 'trust.9921', 'Is.9937', 'have.9940', 'enjoy.9941', 'needed.9943', 'explained.9946', 'explained.9947', 'have.9961', 'make.9962', 'have.9973', 'phoned.9995', 'are.10009', 'have.10010', 'has.10012', 'Love.10021', 'puts.10022', 'charge.10025', 'taste.10026', 'love.10032', 'compares.10033', 'have.10034', 'makes.10039', 'paid.10045', 'want.10047', 's.10069', 'love.10079', 'called.10095', 'have.10102', 'seemed.10105', 'got.10109', 'accepted.10110', 'ended.10113', 'order.10117', 'told.10121', 'are.10123', 'removed.10138', 'knows.10143', 'continues.10152', 'offer.10157', 'go.10162', 'love.10163', 'promise.10166', 'gave.10169', 'watering.10176', 'is.10184', 'sat.10198', 'needs.10206', 'is.10211', 'choose.10221', 'fixeded.10228', 'have.10237', 'came.10242', 'came.10246', 'spent.10249', 'think.10250', 'avoided.10272', 'set.10280', 'had.10284', 'hope.10285', 'called.10287', 'has.10295', 'visited.10302', 'think.10304', 'wanted.10308', 'rocked.10309', 'ordered.10312', 'needs.10323', 'ate.10325', 'tasted.10332', 'had.10338', 'thank.10343', 'Suck.10344', 'blows.10348', 'live.10349', 'help.10352', 'came.10355', 'fixed.10368', 'are.10370', 'had.10371', 'vomited.10373', 'came.10379', 'did.10386', 'was.10393', 'LOVE.10396', 'asked.10404', 'had.10406', 'googled.10407', 'had.10430', 'enjoy.10431', 'reccomend.10432', 'refused.10438', 'had.10452', 'moved.10455', 'used.10456', 'plans.10459', 'have.10462', 'bites.10466', 'is.10473', 'interviewed.10491', 'adopted.10497', 'has.10500', 'wish.10510', 'guess.10516', 'called.10522', 'told.10523', 'makes.10542', 'had.10559', 'prefer.10581', 'ordered.10594', 'asked.10595', 'found.10602', 'amazes.10607', 'has.10610', 'was.10613', 'know.10614', 'love.10616', 'came.10619', 'know.10622', 'made.10625', 'know.10628', 'went.10637', 'found.10639', 'made.10641', 'appreciated.10643', 'have.10647', 'called.10654', 'left.10656', 'had.10657', 'used.10660', 'makes.10668', 'made.10669', 'has.10676', 'love.10679', 'told.10691', 'shows.10699', 'was.10707', 'broke.10712', 'squeezed.10713', 'kept.10716', 'give.10717', 'SEEMS.10725', 'HAVE.10728', 'LOVE.10730', 'continues.10735', 'has.10743', 'arrived.10748', 'stated.10756', 'love.10758', 'makes.10761', 'suck.10773', 'rated.10778', 'is.10781', 'prefer.10782', 'purchased.10794', 'tried.10798', 'have.10801', 'had.10804', 'know.10807', 'was.10811', 'attended.10818', \"'s.10819\", 'allowed.10830', 'knew.10835', 'kept.10842', 'Rocks.10845', 'work.10847', 'have.10872', 'is.10873', 'broke.10889', 'attempt.10897', 'want.10900', 'sent.10904', 'MAKE.10913', 'said.10918', 'ended.10920', 'did.10921', \"'s.10922\", 'have.10932', 'sent.10934', 'feel.10936', 'offer.10945', 'sell.10954', 'took.10955', 'wish.10967', 'delivers.10989', 'know.10990', 'love.10997', 'know.11004', 'leaves.11006', 'were.11018', 'hope.11023', 'took.11037', 'went.11038', 'wanted.11043', 'enjoyed.11045', 'got.11057', 'did.11058', 'looks.11060', 'add.11072', 'put.11081', 'work.11095', 'guided.11099', 'found.11100', 'left.11106', 'explained.11109', 'told.11111', 'gave.11112', 'urge.11114', 'wanted.11118', 'determined.11119', 'dropped.11121', 'hit.11125', 'demanded.11137', 'believe.11138', 'think.11140', 'wish.11145', 'continued.11147', 'loves.11161', 'explained.11169', 'recommended.11183', 'think.11193', 'reeks.11195', 'used.11198', 'have.11201', 'get.11202', 'had.11211', 'where.11217', 'mean.11220', 'googled.11223', 'said.11225', 'enjoyed.11226', 'had.11232', 'said.11240', 'had.11248', 'recommend.11256', 'looks.11264', 'seemed.11268', 'have.11274', 'make.11277', 'moved.11289', 'interviewed.11297', 'shows.11298', 'warned.11318', 'hate.11320', 'killed.11323', 'took.11325', 'live.11326', 'had.11334', 'used.11339', 'attempted.11347', 'arrived.11348', 'refused.11353', 'told.11355', 'have.11360', 'offer.11370', 'guess.11382', 'tried.11383', 'had.11390', 'is.11392', 'had.11393', 'has.11400', 'cover.11401', 'purchased.11404', 'found.11405', 'admit.11408', 'told.11411', 'knows.11412', 'had.11425', 'had.11428', 'go.11433', 'decided.11434', 'have.11436', 'had.11445', 'found.11446', 'had.11450', 'needed.11477', 'were.11486', 'put.11487', 'had.11491', 'was.11497', 'worked.11498', 'know.11500', 'came.11514', 'phoned.11515', 'had.11520', 'asked.11522', 'left.11525', 'started.11531', 'claimed.11537', 'wonder.11539', 'told.11540', 'finished.11542', 'found.11543', 'want.11548', 'looked.11554', 'signed.11559', 'were.11560', 'said.11561', 'had.11564', 'is.11567', 'waited.11572', 'were.11573', 'beg.11581', 'understand.11585', 'asked.11594', 'stopped.11602', 'scared.11603', 'made.11608', 'got.11615', 'have.11623', 'had.11624', 'had.11630', 'paid.11631', 'said.11633', 'has.11634', 'refused.11635', 'think.11641', 'have.11644', 'needed.11645', 'did.11646', 'sent.11648', 'called.11649', 'ordered.11651', 'recommend.11653', 'have.11654', 'made.11656', 'paid.11657', 'said.11658', 'took.11661', 'called.11662', 'gave.11673', 'ended.11674', 'is.11681', 'has.11688', 'enjoyed.11704', 'had.11705', 'went.11710', 'is.11711', 'rent.11714', \"'s.11716\", 'love.11721', 'have.11725', 'expected.11736', 'seem.11750', 'deserves.11759', 'experienced.11763', 'is.11769', 'talks.11770', 'manged.11778', \"'s.11780\", 'bought.11792', 'means.11798', 'asked.11805', 'took.11806', 'thouhgt.11808', 'suspects.11822', 'are.11824', 'have.11832', 'where.11833', 'think.11842', 'look.11853', 'has.11855', 'is.11863', 'activated.11869', 'called.11872', 'offered.11876', 'began.11880', 'starts.11881', 'have.11896', 'figure.11908', 'brought.11911', 'responded.11929', 'said.11932', 'had.11933', 'came.11935', 'were.11936', 'live.11937', 'said.11940', 'emailed.11944', 'attached.11945', 'thought.11950', 'thought.11951', 'thought.11952', 'saw.11954', 'knew.11955', 'have.11956', 'visited.11964', 'guess.11970', 'were.11972', 'are.11974', 'HAS.11976', 'HAS.11977', 'HAD.11982', 'HAD.11983', 'HAS.11989', 'do.11998', 'runs.12000', 'showed.12008', 'loved.12009', 'took.12010', 'stole.12022', 'went.12025', 'belonged.12026', 'lived.12031', 'worked.12035', 'tracked.12041', 'hope.12045', 'chose.12051', 'preceded.12055', 'think.12059', 'is.12064', 'mean.12066', 'suggest.12073', 'KNOWS.12076', 'knows.12080', 'makes.12083', 'think.12088', 'feel.12091', 'work.12102', 'found.12105', 'found.12106', 'had.12112', 'frequent.12115', 'had.12120', 'have.12123', 'is.12125', 'checked.12132', 'grew.12136', 'made.12140', 'slit.12152', 'heard.12155', 'replaced.12157', 'were.12166', 'seemed.12170', 'think.12175', 'played.12182', 'asked.12183', 'mean.12184', 'stunk.12187', 'called.12188', 'asked.12191', 'had.12195', 'dropped.12217', 'said.12219', 'took.12222', 'took.12223', 'told.12225', 'explained.12226', 'asked.12227', 'told.12228', 'said.12229', 'told.12230', 'are.12232', 'signed.12235', 'need.12238', 'are.12247', 'feel.12249', 'called.12250', 'had.12253', 'wanted.12256', 'kept.12257', 'did.12259', 'expected.12274', 'said.12280', 'LET.12283', 'bought.12285', 'waited.12297', 'kept.12301', 'is.12303', 'ordered.12308', 'showed.12309', 'spent.12312', 'hope.12318', 'had.12326', 'got.12327', 'trust.12331', 'have.12345', 'had.12364', 'said.12375', 'say.12386', 'say.12395', 'laugh.12396', 'said.12398', 'say.12399', 'says.12400', 'says.12403', 'say.12404', 'say.12408', 'says.12412', 'hung.12416', 'noticed.12421', 'called.12422', 'heard.12423', 'GET.12433', 'called.12437', 'had.12439', 'returned.12447', 'put.12460', 'touts.12465', 'is.12471', 'is.12472', 'was.12482', 'guess.12483', 'thought.12496', 'said.12498', 'have.12512', 'got.12527', 'said.12528', 'promised.12533', 'called.12540'])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sub_embs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the output in the dedicated directory\n",
    "output_path = \"..\\\\Data\\\\Extracted_Embeddings\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "with open(os.path.join(output_path, \"target_embs_\")+model_ckp.split(\"/\")[-1]+\".pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(target_embeddings, outfile)\n",
    "\n",
    "with open(os.path.join(output_path, \"target_subembs_\")+model_ckp.split(\"/\")[-1]+\".pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(total_sub_embs, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_ckp = \"EleutherAI/pythia-70m-deduped\"\n",
    "model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.46MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.41MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 339kB/s]\n",
      "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m GPT2Tokenizer, GPT2Model\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m GPT2Model\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mgpt2\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mReplace me by any text you\u001b[39m\u001b[39m'\u001b[39m\u001b[39md like.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m encoded_input \u001b[39m=\u001b[39m tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\paperLeL\\Lib\\site-packages\\transformers\\modeling_utils.py:2695\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2680\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2681\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m   2683\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcache_dir\u001b[39m\u001b[39m\"\u001b[39m: cache_dir,\n\u001b[0;32m   2684\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mforce_download\u001b[39m\u001b[39m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2693\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m: commit_hash,\n\u001b[0;32m   2694\u001b[0m     }\n\u001b[1;32m-> 2695\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs)\n\u001b[0;32m   2697\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   2698\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   2699\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[0;32m   2700\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\paperLeL\\Lib\\site-packages\\transformers\\utils\\hub.py:428\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    426\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    427\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 428\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    429\u001b[0m         path_or_repo_id,\n\u001b[0;32m    430\u001b[0m         filename,\n\u001b[0;32m    431\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    432\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    433\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    434\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    435\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    436\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    437\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    438\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    439\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m    440\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[0;32m    442\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    443\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\paperLeL\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\paperLeL\\Lib\\site-packages\\huggingface_hub\\file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1361\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[0;32m   1362\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[1;32m-> 1364\u001b[0m     http_get(\n\u001b[0;32m   1365\u001b[0m         url_to_download,\n\u001b[0;32m   1366\u001b[0m         temp_file,\n\u001b[0;32m   1367\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1368\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1369\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1370\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[0;32m   1371\u001b[0m     )\n\u001b[0;32m   1373\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\paperLeL\\Lib\\site-packages\\huggingface_hub\\file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[0;32m    531\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(…)\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    534\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    535\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    539\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    540\u001b[0m )\n\u001b[1;32m--> 541\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[0;32m    542\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    543\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\paperLeL\\Lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\paperLeL\\Lib\\site-packages\\urllib3\\response.py:940\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 940\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    942\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    943\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\paperLeL\\Lib\\site-packages\\urllib3\\response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[0;32m    877\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[1;32m--> 879\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[0;32m    881\u001b[0m flush_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    882\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\paperLeL\\Lib\\site-packages\\urllib3\\response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    811\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    813\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 814\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[0;32m    816\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    817\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    822\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\paperLeL\\Lib\\site-packages\\urllib3\\response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    797\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\paperLeL\\Lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\paperLeL\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\paperLeL\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\paperLeL\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperLeL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
